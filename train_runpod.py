"""
Qwen 2.5 7B Fine-tuning –¥–ª—è Warhammer 40K
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è RunPod (RTX 3090/4090)
"""

from unsloth import FastLanguageModel
from datasets import load_from_disk
import torch
from trl import SFTTrainer
from transformers import TrainingArguments
import config

print("\n" + "="*60)
print("ü¶• Unsloth + Qwen 2.5 7B Fine-tuning")
print("="*60)

# –§—É–Ω–∫—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
def formatting_func(examples):
    texts = []
    for instruction, input_text, output in zip(examples['instruction'], examples['input'], examples['output']):
        text = f"""### Instruction:
{instruction}

### Input:
{input_text}

### Response:
{output}"""
        texts.append(text)
    return texts

print("\n[1/6] –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Qwen 2.5 7B...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=config.MODEL_PATH,
    max_seq_length=16384,  # 16K –∫–æ–Ω—Ç–µ–∫—Å—Ç (–ø–æ–∫—Ä—ã–≤–∞–µ—Ç 82.7% –∏—Å—Ç–æ—Ä–∏–π)
    dtype=None,  # Auto-detect
    load_in_4bit=True,  # 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM
)

print("\n[2/6] –î–æ–±–∞–≤–ª–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤...")
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
    random_state=3407,
    use_rslora=False,
    loftq_config=None
)

print("\n[3/6] –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
dataset = load_from_disk("warhammer_dataset")
print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
print(f"   üìä –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: ~60K —Å–∏–º–≤–æ–ª–æ–≤/–∏—Å—Ç–æ—Ä–∏—è")

print("\n[4/6] –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç—Ä–µ–Ω–µ—Ä–∞...")
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    formatting_func=formatting_func,
    max_seq_length=16384,
    dataset_num_proc=2,  # RunPod –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç multiprocessing
    packing=False,
    args=TrainingArguments(
        per_device_train_batch_size=2,  # –î–ª—è 24GB VRAM
        gradient_accumulation_steps=4,  # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch = 8
        warmup_steps=5,
        max_steps=100,
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs"
    )
)

print("\n[5/6] –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ...")
print("="*60)
print(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
print(f"   ‚Ä¢ –î–∞—Ç–∞—Å–µ—Ç: {len(dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
print(f"   ‚Ä¢ –ö–æ–Ω—Ç–µ–∫—Å—Ç: 16,384 —Ç–æ–∫–µ–Ω–∞ (~45K —Å–∏–º–≤–æ–ª–æ–≤)")
print(f"   ‚Ä¢ Batch size: 2 √ó 4 = 8 (effective)")
print(f"   ‚Ä¢ –®–∞–≥–æ–≤: 100 (~7.7 —ç–ø–æ—Ö)")
print(f"   ‚Ä¢ VRAM: ~14-16GB")
print(f"   ‚Ä¢ –í—Ä–µ–º—è: ~45-90 –º–∏–Ω—É—Ç")
print(f"   ‚Ä¢ –°—Ç–æ–∏–º–æ—Å—Ç—å: ~$0.26-0.52 (RTX 3090)")
print("="*60 + "\n")

# –û–ë–£–ß–ï–ù–ò–ï
trainer.train()

print("\n[6/6] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
model.save_pretrained(config.FINETUNED_MODEL_PATH)
tokenizer.save_pretrained(config.FINETUNED_MODEL_PATH)

print("\n" + "="*60)
print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
print("="*60)
print(f"üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config.FINETUNED_MODEL_PATH}/")
print("\nüíæ –°–∫–∞—á–∞–π—Ç–µ –ø–∞–ø–∫—É —Å –º–æ–¥–µ–ª—å—é —á–µ—Ä–µ–∑ RunPod interface")
print("   –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: zip -r fine_tuned_model.zip fine_tuned_model/")
print("="*60)


from unsloth import FastLanguageModel
import torch
import config

# ===== –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –î–õ–Ø RTX 3060 TI 12GB =====
print("üîß –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è RTX 3060 Ti 12GB...")
print(f"üìÅ –ü—É—Ç—å: {config.FINETUNED_MODEL_PATH}")

model, tokenizer = FastLanguageModel.from_pretrained(
    config.FINETUNED_MODEL_PATH,
    max_seq_length=16384,  # ‚úÖ –ö–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ (–ø–æ–∫—Ä—ã–≤–∞–µ—Ç 82.7% –∏—Å—Ç–æ—Ä–∏–π)
    dtype=None,
    load_in_4bit=True      # ‚úÖ –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ! –ò–Ω–∞—á–µ –Ω–µ –≤–ª–µ–∑–µ—Ç –≤ 12GB
)
FastLanguageModel.for_inference(model)

# –û—á–∏—Å—Ç–∫–∞ VRAM –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
torch.cuda.empty_cache()

print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. VRAM: {torch.cuda.memory_allocated()/1024**3:.2f}GB / 12GB")
print("")

prompt = """Write an epic Warhammer 40,000 story.

Theme: A battle between Space Marines and Orks on a forgotten planet.
Style: Epic, dramatic, with detailed combat scenes.
Length: Extended narrative, minimum 5000 words.

Story:"""
inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

print("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ Warhammer 40,000...")
print("‚è±Ô∏è  –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: 5-15 –º–∏–Ω—É—Ç (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–ª–∏–Ω—ã)")
print(f"üéØ –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤: 12000 (~35-40K —Å–∏–º–≤–æ–ª–æ–≤)")
print("")

outputs = model.generate(
    **inputs, 
    max_new_tokens=12000,   # ‚úÖ ~35-40K —Å–∏–º–≤–æ–ª–æ–≤ (–±–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è 12GB)
    temperature=0.8,        # –ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
    top_p=0.95,            # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
    do_sample=True,
    use_cache=True         # ‚úÖ –ö–µ—à–∏—Ä–æ–≤–∞—Ç—å –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
)
story = tokenizer.decode(outputs[0])

# –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
torch.cuda.empty_cache()

print("\n" + "="*60)
print("GENERATED STORY")
print("="*60 + "\n")
print(story)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
with open("generated_story.txt", "w", encoding="utf-8") as f:
    f.write(story)

print("\n" + "="*60)
print(f"‚úÖ –ò—Å—Ç–æ—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: generated_story.txt")
print(f"üìä –î–ª–∏–Ω–∞: {len(story)} —Å–∏–º–≤–æ–ª–æ–≤")
print(f"üíæ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ VRAM: {torch.cuda.max_memory_allocated()/1024**3:.2f}GB")
print("="*60)